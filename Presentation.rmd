---
title: "Group 10 Presentation"
author: "Group 10"
date: "18th May 2021"
output: 
  xaringan::moon_reader:
    seal: true # show a title slide with YAML information
    nature:
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      autoplay: 20000
      countdown: 20000 # Can use the countdown to practice
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem

<!-- The prevalence of diabetes is growing rapidly all over the world. -->

<!-- Hospitalized diabetes patients have a higher risk of hospital readmission after discharge from hospital. -->

<!-- This generates significant burden to the patients and the health care system. -->

<!-- Identifying this group of patients will improve the management of the high readmission rate. -->


```{r, out.width = "700px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/Problem.png")
```


---


## Slide 2

Introduction of the dataset - Jenny

---


## Slide 3

Challenge of data - Jenny

---


## IDA - Missingness, Outlier, Correlation and Low information features

```{r, out.width = "900px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/IDA_slide.png")
```

---


## Slide 5

IDA - data cleaning - Leonard

---


## Imbalanced Class

```{r, out.width = "900px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/Imbalanced_class_slide.png")
```

---


## Treating Imbalanced Class

```{r, out.width = "900px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/down_over_sampling_slide.png")
```

---

## Feature Selection

```{r, out.width = "900px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/Feature_selection_slide.png")
```

---

## *k*-nearest neighbours tuning

<!-- Speech (to delete): We first consider the k Nearest Neighbours method. -->
<!-- To tune the hyperparameter k for best accuracy while minimizing computational time, we adopt the heuristic that the best k is approximately equal to root N where N is the number of points. -->
<!-- By iterating around this value, we obtain our best k estimate. -->

```{r, out.width = "900px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/knn_method.png")
```

---

## *k*-nearest neighbours results

<!-- Looking at this heatmap visualisation of the confusion matrix, we find that the kNN model fails to predict any instances of the ‘<30’ class in the test set.  -->
<!-- Second, the model fails to be sensitive to readmissions, which is critical to the goal of our project. -->

```{r, out.width = "1200px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/CM_plot_table.png")
```

---

## Random Forest Methodology

Model 2 - Random Forest
Methodology

<!-- Then we built a random forest model, we firstly tried different dataset to balance the class, and found only down sample data was able to pick up the minority class group. Then we tried different functions and features, and performed hyper-parameter tuning. (with or without: The red highlights indicate tests with best performance.) -->

```{r, out.width = "1200px", echo=FALSE, fig.align="center"}
knitr::include_graphics("Images/Random forest methodology.png")
```

---

## Slide 12

Model 2 - Random Forest
Results

---

## Slide 13

Model 3 - SVM
Methodology

---

## Slide 14

Model 3 - SVM
Results

---

## Slide 15

Model 4 - Logistic Regression
Methodology

---

## Slide 16

Model 4 - Logistic Regression
Results

---

## Slide 17

Model 5 - Decision Tree/GBM
Methodology

---

## Slide 18

Model 5 - Decision Tree/GBM
Results

---

## Slide 19

Conclusion - which model performs the best - Leonard

---

## Slide 20

Future improvement - Jenny and Hui





