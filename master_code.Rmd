---
title: "Group 10 STAT5003 Assignment Report"
author: "Group 10"
date: "`r Sys.Date()`"
geometry: margin=0.5in
output:
  html_document: default
---
\vspace{-5truemm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
seed.value <- 500310
set.seed(seed.value)

install.custom <- function(package.name){
  if(!(package.name %in% row.names(installed.packages()))){
    install.packages(package.name)
  } else {
    print(paste0(package.name, " is already installed."))
  }
  library(package.name, character.only = T)
}

install.custom("purrr")
package_list <- c("broom"
                  , "ggplot2"
                  ,"tidyverse"
                  , "lubridate"
                  , "knitr"
                  , "janitor"
                  , "gridExtra"
                  , "sf"
                  , "dplyr"
                  , "ggpubr"
                  , "patchwork"
                  , "corrplot"
                  , "ggcorrplot"
                  , "matrixStats"
                  , "naniar"
                  , "devtools"
                  , "datapasta" 
                  , "data.table"
                  , "dtplyr"
                  #, "tidymodels"
                  , "xgboost"
                  #, "vip"
                  #, "GGally"
                  , "glmnet"
                  #, "ggthemes"
                  #, "tidytext"
                  , "rmarkdown"
                  #, "openxlsx"
                  #, "xlsx"
                  , "Hmisc"
                  , "mice"
                  , "corrplot"
                  #, "missForest"
                  , "tidyverse"
                  , "caret"
                  , "smotefamily"
                  , "dplyr"
                  )

map(package_list, ~install.custom(.x))
```

# Train/Test Split
Cleaned data was imported and train/test split was performed where 80% will go to train and 20% will go to test. 

```{r, echo = F}
df = read.csv('diabetic_data_clean_S1.csv', header = TRUE)
df  <- select(df, -1) #remove row number
df <- df %>% select(-c('encounter_id', 'patient_nbr'))
```

```{r, echo = F}
inTrain <- createDataPartition(df$readmitted, p = .8)[[1]]
train <- df[ inTrain, ]
test  <- df[-inTrain, ]
```

# Treating Imbalanced Class

One of the issues about the dataset is imbalanced class. From the bar plot below it can be observed that the class variable is dominated by the 'NO' level. If this issue is not treated properly, the trained classifier will tend to be more biased towards the major class and potentially not making any predictions on the minor classes. 

Therefore, to treat the imbalanced class issue, several techniques were perform as follows:
1. Oversampling by replicating instances from minor classes
2. Oversampling by SMOTE 
3. Downsampling

With oversampling by replicating instances from minor classes, we first subset for the two minor classes: <30 and >30. Then we replicated instances 5 times and 2 times respectively for each class, in order to make the dataset more balanced. In this way, we didn't introduce any artificial instances but this is likely to result in overfitting where training accuracy maybe high but the model is not good at predicting for unseen data. 

```{r, echo = F}
#oversampling - replicating minority classes
train.class1.raw <- subset(train, train$readmitted == '<30') # <30
train.class2.raw <- subset(train,train$readmitted == 'NO') 
train.class3.raw <- subset(train, train$readmitted == '>30') # >30

#replicate class 1 5 times and class 3 1 time
class1.rows= c(1:nrow(train.class1.raw))
class1.times = 5
train.class1.raw.rep <- train.class1.raw[rep(class1.rows,class1.times),]

class3.rows = c(1:nrow(train.class3.raw))
class3.times = 2
train.class3.raw.rep <- train.class3.raw[rep(class3.rows,class3.times),]

df_oversampled_rep <- rbind(train.class1.raw.rep, train.class2.raw, train.class3.raw.rep)

#table(df_oversampled_rep$readmitted)
```

We also explored the option of SMOTE to oversample the minor classes. Instead of duplicating instances from the minority class that may lead to overfitting, SMOTE synthesizes new instances of the minority class between a random instance of the minority class and one of its k nearest neighbor (here K = 3). Given that most features in the data were categorical, one hot encoding was first performed. Since SMOTE generates new instances by calculating the distance between the random data point of the minority class and a random point of its neighbour, the raw results of new instances from SMOTE will generate decimals between 0 and 1. This adds no meaingful value to the data, therefore, an ifelse rule was created to round >0.5 to 1 and <0.5 to 0. Checks were also performed to ensure for each predictor, only one level will be 1 and the rest of the levels are 0. For new numerical instances, we also rounded the values to integer, aligning with the original data format. 

```{r, echo = F}
#one hot encoding
#library(caret)
dummy <- dummyVars(" ~ .", data=train)
oh.df.train <- data.frame(predict(dummy, newdata = train))
```

```{r, echo = F}
#training oversampling - SMOTE
train.class1 <- subset(oh.df.train, oh.df.train$readmitted.30 == 1) #<30
train.class2 <- subset(oh.df.train, oh.df.train$readmittedNO == 1)
train.class3 <- subset(oh.df.train, oh.df.train$readmitted.30.1 == 1) # >30


new.train.class.1 <- SMOTE(train.class1,train.class1[,"readmitted.30"],K =3, dup_size = 4)
new.train.class.3 <- SMOTE(train.class3,train.class3[,"readmitted.30.1"],K =3, dup_size = 1)

new.train.class.1 <- new.train.class.1$data
new.train.class.3 <- new.train.class.3$data

new.train.class.1 <- new.train.class.1[,-ncol(new.train.class.1)]
new.train.class.3 <- new.train.class.3[,-ncol(new.train.class.3)]
```

```{r, echo = F}
df_smote_train <- rbind(new.train.class.1, train.class2, new.train.class.3 )

#getting label back
#class1
class1 <- subset(df_smote_train, df_smote_train$readmitted.30 == 1) #<30 
class1 <- class1 %>% select(-c('readmitted.30.1', 'readmittedNO', 'readmitted.30'))
class1$readmitted <- "<30"

#class2
class2 <- subset(df_smote_train, df_smote_train$readmittedNO == 1) #NO
class2 <- class2 %>% select(-c('readmitted.30.1', 'readmitted.30','readmittedNO'))
class2$readmitted <- "NO"

#class3
class3 <- subset(df_smote_train, df_smote_train$readmitted.30.1 == 1) #>30
class3 <- class3 %>% select(-c('readmitted.30.1', 'readmitted.30','readmittedNO'))
class3$readmitted <- ">30"

#combine 3 classes with un-encoded labels
df_smote_train_FINAL <- rbind(class1, class2, class3)

#table(df_smote_train_FINAL$readmitted)
```


```{r, echo = F}
#rounding for new instances
#colnames(df_smote_train_FINAL)

#for categorical factors that don't need rounding
for (i in c(1:15, 20:46, 48:87)){
  df_smote_train_FINAL[,i] <- ifelse(df_smote_train_FINAL[,i] >0.5,1,0)
}

#rounding numerical features to integer
for (i in c(16:19, 47)){
  df_smote_train_FINAL[,i] <- round(df_smote_train_FINAL[,i],0)
}
```


```{r, echo = F}
#doing checks to see if the rounding works
#sum(rowSums(df_smote_train_FINAL[,88:90])) #this sum should equal to the number of rows
#check <- df_smote_train_FINAL[rowSums(df_smote_train_FINAL[,1:3]) != 1,]
```

After oversampling techniques, the class distribution of the new data is approximately 4:3:3, which yields a more balanced dataset. 
We also explored the option of downsampling, we first converted categorical predictors to factors as a preliminary feature engineering step for future model training, then we applied the downSample function to downsample the major classes 'NO' and '>30' class so that 3 classes are now balanced. However, although we didn't introduce new or artificial instances, we may potentially lose some useful information.

```{r, echo = F}
#DOWNSAMPLING
# Convert character to factor in train data
train$race <- factor(train$race)
train$gender <- factor(train$gender)
train$age <- factor(train$age, order = TRUE, levels = c("[0-10)", "[10-20)", "[20-30)", "[30-40)", "[40-50)", "[50-60)", "[60-70)", "[70-80)", "[80-90)", "[90-100)"))
train$diag_1 <- factor(train$diag_1 )
train$diag_2 <- factor(train$diag_2)
train$diag_3 <- factor(train$diag_3)
train$A1Cresult <- factor(train$A1Cresult)
train$metformin <- factor(train$metformin)
train$glipizide <- factor(train$glipizide)
train$glyburide <- factor(train$glyburide)
train$pioglitazone <- factor(train$pioglitazone)
train$rosiglitazone <- factor(train$rosiglitazone)
train$insulin  <- factor(train$insulin )
train$change <- factor(train$change)
train$diabetesMed <- factor(train$diabetesMed)
train$readmitted <- factor(train$readmitted, order = TRUE, levels = c("NO", ">30", "<30"))
train$admission_source <- factor(train$admission_source)
train$discharge_disposition <- factor(train$discharge_disposition)
train$admission_type <- factor(train$admission_type)
```

```{r, echo = F}
#Downsampling
train_down <- downSample(subset(train, select = -c(readmitted)), train[["readmitted"]], list = FALSE)

names(train_down)[names(train_down) == "Class"] <- "readmitted"

#table(train_down$readmitted)
```

```{r, echo = F, fig.width=8, fig.height=2.5}
#producing graphs for 3 re-sampling techniques
par(mfrow = c(1,4))

barplot(prop.table(table(train$readmitted
)),
  xlab = "Class",
  main = "Original Class Distribution", cex.main = 0.8, 
  cex.axis = 0.6, cex.lab = 0.8)

barplot(prop.table(table(df_oversampled_rep$readmitted
)),
  xlab = "Class",
  main = "Oversampling by replication", cex.main = 0.8, 
  cex.axis = 0.6, cex.lab = 0.8)

barplot(prop.table(table(df_smote_train_FINAL$readmitted
)),
  xlab = "Class",
  main = "Oversampling by SMOTE", cex.main = 0.8, 
  cex.axis = 0.6, cex.lab = 0.8)

barplot(prop.table(table(train_down$readmitted
)),
  xlab = "Class",
  main = "Downsampling", cex.main = 0.8, 
  cex.axis = 0.6, cex.lab = 0.8)

```


```{r, echo = F}
#export to csv
#testing
#write.csv(test,"test_raw.csv", row.names = FALSE)

#training
#write.csv(train,"train_raw.csv", row.names = FALSE)
#write.csv(df_oversampled_rep,"train_rep_oversampled.csv", row.names = FALSE)
#write.csv(df_smote_train_FINAL,"train_SMOTE_oversampled.csv", row.names = FALSE)
#write.csv(train_down,"train_down.csv", row.names = FALSE)
```

# Feature Engineering

After performing train/test split and re-sampling, feature selection was performed to identify the potential subset of important features. Mutinomial Lasso regression was chosen as the feature selection algorithm as we aim to generate a simpler model with less features. Unlike Ridge regression that puts less weight on less important features, Lasso regression obtain the subset of predictors that minimizes prediction error by adding a regularization term to the cost function so coefficients for some variables are shrunk towards zero and being completely ignored.

A grid was generated to identify the best lambda in Lasso as we aim to find the balance between simplicity and model fit that minimizes the mse in cross validation. A 10-fold cross-validation for Lasso was performed with standardization to address the issue that variables are sharing different units. In the result, the best lambda that minimizes the cross-validation error was 0.0006. 

```{r , echo = F, fig.show="hide"}
# train/test split
inTrain <- createDataPartition(df$readmitted, p = .8)[[1]]
train <- df[ inTrain, ]
test  <- df[-inTrain, ]

# Selecting the Numerical Columns
num_cols <- unlist(lapply(train, is.numeric))
num_cols <- names(num_cols[which(num_cols)])

# Selecting Categorical Columns
cat_cols <- unlist(lapply(train, is.character))
cat_cols <- names(cat_cols[which(cat_cols)])


# Label Encoding
encode_ordinal <- function(x, order = unique(x)) {
  x <- as.numeric(factor(x, levels = order, exclude = NULL))
  x
}

for (i in 1:length(cat_cols)){
  train[[cat_cols[i]]] <- encode_ordinal(train[[cat_cols[i]]])
}


df_x <- train %>% select(-c('readmitted'))
df_y <- train %>% select(c('readmitted'))

x <- as.matrix(df_x)
y <- as.double(as.matrix(df_y))

#set alpha = 1 for Lasso
#multinomial for multi class classification
grid <- 10^seq(8,-2, length=100)

cv.out <- cv.glmnet(x, y, family = 'multinomial', alpha=1, standardize = TRUE, type.measure = 'mse')

plot(cv.out)
```

```{r, echo = F}
#choosing best lambda
bestlam <- cv.out$lambda.min
#bestlam
```

A subset of features were selected by Lasso. Lasso shrinks the model down from 23 predictors to 12 predictors and identifying race, gender, age, diag_3, number_diagnoses, admissiong_source, discharge_disposition, admission_type and some other medications as important features. However, one of the pitfalls of Lasso regression is that it does not provide much interpretability on why such features were selected, hence trial and error was conducted in later modelling stage to test if using Lasso selected features only will yield a better model against using all features.

```{r, echo = F}
# subset of features
coef_names_lasso <- coef(cv.out, s = 0)[[1]]
selected_variables <- row.names(coef_names_lasso)[coef_names_lasso@i+1]
#selected_variables
```


