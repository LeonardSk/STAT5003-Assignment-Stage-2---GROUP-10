---
title: "Group 10 STAT5003 Assignment Stage 2"
author: "Group 10"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
seed.value <- 500310
set.seed(seed.value)

install.custom <- function(package.name){
  if(!(package.name %in% row.names(installed.packages()))){
    install.packages(package.name)
  } else {
    print(paste0(package.name, " is already installed."))
  }
  library(package.name, character.only = T)
}

install.custom("purrr")
package_list <- c("broom"
                  , "ggplot2"
                  ,"tidyverse"
                  , "lubridate"
                  , "knitr"
                  , "janitor"
                  , "gridExtra"
                  , "sf"
                  , "dplyr"
                  , "ggpubr"
                  , "patchwork"
                  , "corrplot"
                  , "ggcorrplot"
                  , "matrixStats"
                  , "naniar"
                  , "devtools"
                  , "datapasta" 
                  , "data.table"
                  , "dtplyr"
                  #, "tidymodels"
                  , "xgboost"
                  #, "vip"
                  #, "GGally"
                  , "glmnet"
                  #, "ggthemes"
                  #, "tidytext"
                  , "rmarkdown"
                  #, "openxlsx"
                  #, "xlsx"
                  , "Hmisc"
                  , "mice"
                  , "corrplot"
                  #, "missForest"
                  , "tidyverse"
                  , "caret"
                  , "smotefamily"
                  , "dplyr"
                  )

map(package_list, ~install.custom(.x))
```

## Train/Test Split
Import cleaned data and performed train/test split where 80% will go to train and 20% will go to test. 

```{r}
df = read.csv('diabetic_data_clean_S1.csv', header = TRUE)
df  <- select(df, -1) #remove row number
df <- df %>% select(-c('encounter_id', 'patient_nbr'))
```

```{r}
inTrain <- createDataPartition(df$readmitted, p = .8)[[1]]
train <- df[ inTrain, ]
test  <- df[-inTrain, ]
```

## Oversampling by replicating instances from minority class

As identified in previous sections, one of the issues about the dataset is imbalanced class. ">30" and "<30" were undersampled. If the class is imbalanced, it is highly likely that the predicted class would be dominated by the majority class, which leads to a biased model result.
Therefore, Oversampling was performed in two approaches. The first one we chose to replicate the instances from the minority classes generated a new class distribution of 4:3:3. Likely this will lead to overfitting where training accuracy maybe high but the model is not good at predicting for unseen data. 

```{r}
#oversampling - replicating minority classes
train.class1.raw <- subset(train, train$readmitted == '<30') # <30
train.class2.raw <- subset(train,train$readmitted == 'NO') 
train.class3.raw <- subset(train, train$readmitted == '>30') # >30

#replicate class 1 5 times and class 3 1 time
class1.rows= c(1:nrow(train.class1.raw))
class1.times = 5
train.class1.raw.rep <- train.class1.raw[rep(class1.rows,class1.times),]

class3.rows = c(1:nrow(train.class3.raw))
class3.times = 2
train.class3.raw.rep <- train.class3.raw[rep(class3.rows,class3.times),]

df_oversampled_rep <- rbind(train.class1.raw.rep, train.class2.raw, train.class3.raw.rep)

table(df_oversampled_rep$readmitted)
```

## Oversampling by SMOTE

Therefore, another technique called SMOTE was performed to oversample the minority classes. Instead of duplicating instances from the minority class that may lead to overfitting, SMOTE synthesizes new instances of the minority class between a random instance of the minority class and one of its k nearest neighbor (here K = 3). Given that most features in the data were categorical, one hot encoding was first performed. Since SMOTE generates new instances by calculating the distance between the random data point of the minority class and a random point of its neighbour, the raw results from SMOTE will contain decimals between 0 and 1. This adds no meaingful value to the data, therefore, an ifelse rule was created to round >0.5 to 1 and <0.5 to 0. 
After applying SMOTE, the class distribution of the new data is approximately 4:3:3, which yields a more balanced dataset. 

```{r}
#one hot encoding
#training 
library(caret)
dummy <- dummyVars(" ~ .", data=train)
oh.df.train <- data.frame(predict(dummy, newdata = train))
```

```{r}
#training oversampling - SMOTE
train.class1 <- subset(oh.df.train, oh.df.train$readmitted.30 == 1) #<30
train.class2 <- subset(oh.df.train, oh.df.train$readmittedNO == 1)
train.class3 <- subset(oh.df.train, oh.df.train$readmitted.30.1 == 1) # >30


new.train.class.1 <- SMOTE(train.class1,train.class1[,"readmitted.30"],K =3, dup_size = 4)
new.train.class.3 <- SMOTE(train.class3,train.class3[,"readmitted.30.1"],K =3, dup_size = 1)

new.train.class.1 <- new.train.class.1$data
new.train.class.3 <- new.train.class.3$data

new.train.class.1 <- new.train.class.1[,-ncol(new.train.class.1)]
new.train.class.3 <- new.train.class.3[,-ncol(new.train.class.3)]
```

```{r}
df_smote_train <- rbind(new.train.class.1, train.class2, new.train.class.3 )

#getting label back
#class1
class1 <- subset(df_smote_train, df_smote_train$readmitted.30 == 1) #<30 
class1 <- class1 %>% select(-c('readmitted.30.1', 'readmittedNO', 'readmitted.30'))
class1$readmitted <- "<30"

#class2
class2 <- subset(df_smote_train, df_smote_train$readmittedNO == 1) #NO
class2 <- class2 %>% select(-c('readmitted.30.1', 'readmitted.30','readmittedNO'))
class2$readmitted <- "NO"

#class3
class3 <- subset(df_smote_train, df_smote_train$readmitted.30.1 == 1) #>30
class3 <- class3 %>% select(-c('readmitted.30.1', 'readmitted.30','readmittedNO'))
class3$readmitted <- ">30"

#combine 3 classes with un-encoded labels
df_smote_train_FINAL <- rbind(class1, class2, class3)

#table(df_smote_train_FINAL$readmitted)
```


```{r}
#rounding for new instances
#colnames(df_smote_train_FINAL)

#for categorical factors that dont need rounding
for (i in c(1:15, 20:46, 48:87)){
  df_smote_train_FINAL[,i] <- ifelse(df_smote_train_FINAL[,i] >0.5,1,0)
}

#rounding numerical features to integer
for (i in c(16:19, 47)){
  df_smote_train_FINAL[,i] <- round(df_smote_train_FINAL[,i],0)
}
```


```{r}
#doing checks to see if the rounding works
#sum(rowSums(df_smote_train_FINAL[,88:90]))
#check <- df_smote_train_FINAL[rowSums(df_smote_train_FINAL[,1:3]) != 1,]
```


```{r}
par(mfrow = c(1,2))
barplot(prop.table(table(df$readmitted
)),
  xlab = "Class",
  main = "Training data Class Distribution before Oversampling", cex.main = 0.5, 
  cex.axis = 0.9, cex.lab = 0.7)
#checking class distribution after oversampling
barplot(prop.table(table(df_smote_train_FINAL$readmitted
)),
  xlab = "Class",
  main = "Training data Class Distribution after Oversampling", cex.main = 0.5, 
  cex.axis = 0.9, cex.lab = 0.7)
```

## Exporting to CSV

```{r}
#export to csv
#testing
#write.csv(test,"test_raw.csv", row.names = FALSE)

#training
#write.csv(train,"train_raw.csv", row.names = FALSE)
#write.csv(df_oversampled_rep,"train_rep_oversampled.csv", row.names = FALSE)
#write.csv(df_smote_train_FINAL,"train_SMOTE_oversampled.csv", row.names = FALSE)
```

## Feature Selection

Features were first being separated into numerical and categorical. For categorical features, label encoding was performed as a pre-processing step. Label encoding was chosen over one hot encoding because the dataset is large with 19 categorical features and converting all 19 features into one hot encoding will significantly increase the computational time. Performing one hot encoding after feature selection with reduced features will be more appropriate. 

Lasso feature selection technique was performed. 
The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error by adding a regularization term to the cost function so coefficients for some variables are shrunk towards zero and being completely ignored. This yields a simpler model with less features. 

A grid was generated to identify the best lambda in Lasso as we aim to find the balance between simplicity and model fit that minimizes the mse in cross validation. A 10-fold cross-validation for Lasso was performed with standardization to address the issue that variables are sharing different units. In the result, the best lambda that minimizes the cross-validation error was 0.0006. 

```{r}
### ------------------- Lasso Regression 

# Selecting the Numerical Columns
num_cols <- unlist(lapply(df, is.numeric))
num_cols <- names(num_cols[which(num_cols)])

# Selecting Categorical Columns
cat_cols <- unlist(lapply(df, is.character))
cat_cols <- names(cat_cols[which(cat_cols)])


# Label Encoding
encode_ordinal <- function(x, order = unique(x)) {
  x <- as.numeric(factor(x, levels = order, exclude = NULL))
  x
}

for (i in 1:length(cat_cols)){
  df[[cat_cols[i]]] <- encode_ordinal(df[[cat_cols[i]]])
}


df_x <- df %>% select(-c('readmitted'))
df_y <- df %>% select(c('readmitted'))

x <- as.matrix(df_x)
y <- as.double(as.matrix(df_y))

set.seed(123)
#set alpha = 1 for Lasso
#multinomial for multi class classification
grid <- 10^seq(8,-2, length=100)

cv.out <- cv.glmnet(x, y, family = 'multinomial', alpha=1, standardize = TRUE, type.measure = 'mse')

plot(cv.out)
```

```{r}
#choosing best lambda
bestlam <- cv.out$lambda.min
bestlam
```

Finally a subset of features were selected by Lasso. From the list below it can be observed that the subsettd features mostly overlap with the ones that were identified in the GLM variable ranking.
However, different algorithms may generate different variable importance, therefore the subset of important features here identified by Lasso is serving as a starting point only. 

```{r}
# subset of features
coef_names_lasso <- coef(cv.out, s = 0)[[1]]
selected_variables <- row.names(coef_names_lasso)[coef_names_lasso@i+1]

selected_variables
```

